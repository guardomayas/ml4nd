{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofgRAXiPlLLV"
      },
      "source": [
        "# Calcium Deconvolution\n",
        "\n",
        "In this lab you'll write your own code for demixing and deconvolving calcium imaging videos. Demixing refers to the problem of identifying potentially overlapping neurons in the video and separating their fluorescence traces. Deconvolving refers to taking those traces and finding the times of spiking activity, which produce exponentially decaying transients in fluorescence. We'll frame it as a constrained and (partially) non-negative matrix factorization problem, inspired by the CNMF model of Pnevmatikakis et al, 2016, which is implemented in [CaImAn](https://github.com/flatironinstitute/CaImAn) (Giovannucci et al, 2019). More details and further references are in the course notes. We'll use [CVXpy](https://www.cvxpy.org/) to solve the convex optimization problems at the hard of this approach.\n",
        "\n",
        "**References**\n",
        "- Pnevmatikakis, Eftychios A., Daniel Soudry, Yuanjun Gao, Timothy A. Machado, Josh Merel, David Pfau, Thomas Reardon, et al. 2016. “Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data.” Neuron 89 (2): 285–99.\n",
        "[link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4881387/)\n",
        "- Giovannucci, Andrea, Johannes Friedrich, Pat Gunn, Jérémie Kalfon, Brandon L. Brown, Sue Ann Koay, Jiannis Taxidis, et al. 2019. “CaImAn an Open Source Tool for Scalable Calcium Imaging Data Analysis.” eLife. [link](http://dx.doi.org/10.7554/eLife.38173)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UyLyy32uvcu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "try:\n",
        "    import jaxtyping\n",
        "except ImportError:\n",
        "    !pip install jaxtyping\n",
        "\n",
        "from typing import Optional, Any, Dict, Union, Tuple\n",
        "from jaxtyping import Float, Int, Array\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "u2mqV4PPZkpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI-GgS6CWRjY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as dist\n",
        "import numpy as np\n",
        "\n",
        "# We'll use a few SciPy functions too\n",
        "import scipy.sparse\n",
        "from scipy.signal import butter, sosfilt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.feature import peak_local_max\n",
        "\n",
        "# We'll use CVXpy to solve convex optimization problems\n",
        "import cvxpy as cvx\n",
        "\n",
        "# Plotting stuff\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
        "from matplotlib.patches import Circle\n",
        "import seaborn as sns\n",
        "\n",
        "# Helpers\n",
        "from tqdm.auto import trange\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4budK6amAKXQ"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuQdctloR_kS"
      },
      "source": [
        "### Download example data\n",
        "\n",
        "This demo data was contributed by Sue Ann Koay and David Tank (Princeton University).\n",
        "It is also used in the CaImAn demo notebook.\n",
        "We used CaImAn and NoRMCorr to correct for motion artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYoQE7qUmjP2"
      },
      "outputs": [],
      "source": [
        "! wget -nc https://www.dropbox.com/s/8yewyr86wc3tji7/data.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF_4FOm9-ufu"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = torch.load(\"data.pt\")\n",
        "height, width, num_frames = data.shape\n",
        "\n",
        "# Set some constants\n",
        "FPS = 30                        # frames per second in the movie\n",
        "NEURON_WIDTH = 10               # approximate width (in pixels) of a neuron\n",
        "GCAMP_TIME_CONST_SEC = 0.300    # reasonable guess for calcium decay time const."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzhuPnhXYr6L"
      },
      "source": [
        "### Helper functions for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-s9ZoJomK4Q",
        "tags": [
          "hide-cell"
        ]
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for movies and plotting { display-mode: \"form\" }\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from tempfile import NamedTemporaryFile\n",
        "import base64\n",
        "\n",
        "# Set some plotting defaults\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "# initialize a color palette for plotting\n",
        "palette = sns.xkcd_palette([\"windows blue\",\n",
        "                            \"red\",\n",
        "                            \"medium green\",\n",
        "                            \"dusty purple\",\n",
        "                            \"orange\",\n",
        "                            \"amber\",\n",
        "                            \"clay\",\n",
        "                            \"pink\",\n",
        "                            \"greyish\"])\n",
        "\n",
        "_VIDEO_TAG = \"\"\"<video controls>\n",
        " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
        " Your browser does not support the video tag.\n",
        "</video>\"\"\"\n",
        "\n",
        "def _anim_to_html(anim: animation.FuncAnimation,\n",
        "                  fps: int = 20) -> str:\n",
        "    \"\"\"\n",
        "    Convert a Matplotlib animation object to an HTML video snippet\n",
        "    with an embedded base64-encoded video.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    anim : matplotlib.animation.FuncAnimation\n",
        "        The Matplotlib animation object to encode.\n",
        "    fps : int, optional\n",
        "        Frames per second to use when encoding the animation. Default is 20.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        An HTML string containing the video tag with the encoded video.\n",
        "    \"\"\"\n",
        "    if not hasattr(anim, '_encoded_video'):\n",
        "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
        "            anim.save(f.name, fps=fps, extra_args=['-vcodec', 'libx264'])\n",
        "            video = open(f.name, \"rb\").read()\n",
        "        anim._encoded_video = base64.b64encode(video)\n",
        "\n",
        "    return _VIDEO_TAG.format(anim._encoded_video.decode('ascii'))\n",
        "\n",
        "def _display_animation(anim: animation.FuncAnimation,\n",
        "                       fps: int = 30,\n",
        "                       start: int = 0,\n",
        "                       stop: Optional[int] = None) -> HTML:\n",
        "    \"\"\"\n",
        "    Display a Matplotlib animation by converting it to an HTML video snippet\n",
        "    and closing its figure.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    anim : matplotlib.animation.FuncAnimation\n",
        "        The animation object to display.\n",
        "    fps : int, optional\n",
        "        Frames per second for the displayed animation. Default is 30.\n",
        "    start : int, optional\n",
        "        Starting frame index (currently not used #TODO). Default is 0.\n",
        "    stop : Optional[int], optional\n",
        "        Ending frame index (currently not used #TODO). Default is None.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    IPython.display.HTML\n",
        "        An HTML snippet containing the video for display.\n",
        "    \"\"\"\n",
        "    plt.close(anim._fig)\n",
        "    return HTML(_anim_to_html(anim, fps=fps))\n",
        "\n",
        "def play(movie: Int[Tensor, \"height width num_frames\"],\n",
        "         fps: int = FPS,\n",
        "         speedup: int = 1,\n",
        "         fig_height: int = 6):\n",
        "    \"\"\"\n",
        "    Create an animation from a movie tensor and return\n",
        "    an HTML snippet for embedding.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    movie : torch.Tensor\n",
        "        A 3D tensor with shape (height, width, num_frames) for the movie frames.\n",
        "    fps : int, optional\n",
        "        Frames per second of the movie. Default is the global FPS constant.\n",
        "    speedup : int, optional\n",
        "        Factor to speed up the animation. Default is 1 (real-time).\n",
        "    fig_height : int, optional\n",
        "        Height of the figure (in inches) for the plot. Default is 6.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    IPython.display.HTML\n",
        "        An HTML video snippet displaying the animation.\n",
        "\n",
        "    Notes:\n",
        "    ------\n",
        "    This function uses Matplotlib's FuncAnimation to create the animation\n",
        "    and embeds it as an HTML video.\n",
        "    \"\"\"\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    Py, Px, T = movie.shape\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(fig_height * Px/Py, fig_height))\n",
        "    im = plt.imshow(movie[..., 0], interpolation='None', cmap=plt.cm.gray)\n",
        "    tx = plt.text(0.75, 0.05, 't={:.3f}s'.format(0),\n",
        "                  color='white',\n",
        "                  fontdict=dict(size=12),\n",
        "                  horizontalalignment='left',\n",
        "                  verticalalignment='center',\n",
        "                  transform=ax.transAxes)\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i: int):\n",
        "        \"\"\"\n",
        "        Update function for FuncAnimation.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        i : int\n",
        "            The frame index.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the modified image;\n",
        "            used for efficient animation updates.\n",
        "        \"\"\"\n",
        "        im.set_data(movie[..., i * speedup])\n",
        "        tx.set_text(\"t={:.3f}s\".format(i * speedup / fps))\n",
        "        return im,\n",
        "\n",
        "    # call the animator.  blit=True means only re-draw the parts that changed.\n",
        "    anim = animation.FuncAnimation(fig, animate,\n",
        "                                   frames=T // speedup,\n",
        "                                   interval=1,\n",
        "                                   blit=True)\n",
        "    plt.close(anim._fig)\n",
        "\n",
        "    # return an HTML video snippet\n",
        "    print(\"Preparing animation. This may take a minute...\")\n",
        "    return HTML(_anim_to_html(anim, fps=30))\n",
        "\n",
        "def plot_problem_1d(local_correlations: Float[torch.Tensor, \"height width\"],\n",
        "                    filtered_correlations: Float[torch.Tensor, \"height width\"],\n",
        "                    peaks: Int[torch.Tensor, \"num_peaks 2\"]) -> None:\n",
        "    \"\"\"\n",
        "    Plot local correlations, filtered correlations,\n",
        "    and candidate neurons side-by-side.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    local_correlations : torch.Tensor\n",
        "        2D tensor representing local correlation values,\n",
        "        with shape (height, width).\n",
        "    filtered_correlations : torch.Tensor\n",
        "        2D tensor representing filtered correlation values,\n",
        "        with shape (height, width).\n",
        "    peaks : torch.Tensor\n",
        "        Tensor of shape (num_peaks, 2) where each row is [y, x] coordinates\n",
        "        for peak locations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function displays three panels and overlays circles and labels\n",
        "    on the candidate neuron panel.\n",
        "    \"\"\"\n",
        "    def _plot_panel(ax: plt.Axes,\n",
        "                    im: Float[torch.Tensor, \"height width\"],\n",
        "                    title: str) -> None:\n",
        "        \"\"\"\n",
        "        Plot an individual panel with an image and a colorbar.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ax : matplotlib.axes.Axes\n",
        "            The axis on which to plot.\n",
        "        im : torch.Tensor\n",
        "            The image data to display, with shape (height, width).\n",
        "        title : str\n",
        "            Title for the panel.\n",
        "        \"\"\"\n",
        "        h = ax.imshow(im, cmap=\"Greys_r\")\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlim(0, width)\n",
        "        ax.set_ylim(height, 0)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        # add a colorbar of the same height\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
        "        plt.colorbar(h, cax=cax)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
        "    _plot_panel(axs[0], local_correlations, \"local correlations\")\n",
        "    _plot_panel(axs[1], filtered_correlations, \"filtered correlations\")\n",
        "    _plot_panel(axs[2], local_correlations, \"candidate neurons\")\n",
        "\n",
        "    # Draw circles around the peaks\n",
        "    for n, yx in enumerate(peaks):\n",
        "        y, x = yx\n",
        "        axs[2].add_patch(Circle((x, y),\n",
        "                                radius=NEURON_WIDTH/2,\n",
        "                                facecolor='none',\n",
        "                                edgecolor='red',\n",
        "                                linewidth=1))\n",
        "\n",
        "        axs[2].text(x, y, \"{}\".format(n),\n",
        "                    horizontalalignment=\"center\",\n",
        "                    verticalalignment=\"center\",\n",
        "                    fontdict=dict(size=10, weight=\"bold\"),\n",
        "                    color='r')\n",
        "\n",
        "def plot_problem_2(traces: Float[torch.Tensor, \"num_neurons num_frames\"],\n",
        "                     denoised_traces: Float[torch.Tensor, \"num_neurons num_frames\"],\n",
        "                     amplitudes: Float[torch.Tensor, \"num_neurons num_frames\"]) -> None:\n",
        "    \"\"\"\n",
        "    Plot raw and denoised fluorescence traces for neurons\n",
        "    along with amplitude markers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    traces : torch.Tensor\n",
        "        2D tensor of raw fluorescence traces with shape (num_neurons, num_frames).\n",
        "    denoised_traces : torch.Tensor\n",
        "        2D tensor of denoised fluorescence traces, same shape as 'traces'.\n",
        "    amplitudes : torch.Tensor\n",
        "        2D tensor of estimated amplitudes, same shape as 'traces'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Normalizes the traces by their 99.5th percentile,\n",
        "    offsets the traces for clarity, and highlights amplitude markers.\n",
        "    \"\"\"\n",
        "    num_neurons, num_frames = traces.shape\n",
        "\n",
        "    # Plot the traces and our denoised estimates\n",
        "    scale = torch.quantile(traces, .995, dim=1, keepdims=True)\n",
        "    offset = -torch.arange(num_neurons)\n",
        "\n",
        "    # Plot points at the time frames where the (normalized) amplitudes are > 0.05\n",
        "    sparse_amplitudes = amplitudes / scale\n",
        "    # sparse_amplitudes = torch.isclose(sparse_amplitudes, 0, atol=0.05)\n",
        "    sparse_amplitudes[sparse_amplitudes < 0.05] = torch.nan\n",
        "    sparse_amplitudes[sparse_amplitudes > 0.05] = 0.0\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot((traces / scale).T + offset , color=palette[0], lw=1, alpha=0.5)\n",
        "    plt.plot((denoised_traces / scale).T + offset, color=palette[0], lw=2)\n",
        "    plt.plot((sparse_amplitudes).T + offset, color=palette[1], marker='o', markersize=2)\n",
        "    plt.xlabel(\"time (frames)\")\n",
        "    plt.xlim(0, num_frames)\n",
        "    plt.ylabel(\"neuron\")\n",
        "    plt.yticks(-torch.arange(0, num_neurons, step=5),\n",
        "               labels=torch.arange(0, num_neurons, step=5).numpy())\n",
        "    plt.ylim(-num_neurons, 2)\n",
        "    plt.title(\"raw and denoised fluorescence traces\")\n",
        "\n",
        "\n",
        "def plot_problem_3(flat_data: Any,\n",
        "                     params: Dict[str, torch.Tensor],\n",
        "                     hypers: Any,\n",
        "                     plot_bkgd: bool = True,\n",
        "                     indices: Optional[Int[torch.Tensor, \"num_indices\"]] = None) -> None:\n",
        "    \"\"\"\n",
        "    Plot neuron footprints and their corresponding fluorescence traces.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    flat_data : Any\n",
        "        Data used for plotting (currently not used within the function).\n",
        "    params : dict[str, torch.Tensor]\n",
        "        Dictionary containing model parameters with the following keys:\n",
        "          - 'footprints': Neuron footprints, expected to be reshaped to (-1, height, width).\n",
        "          - 'bkgd_footprint': Background footprint reshaped to (height, width).\n",
        "          - 'traces': Fluorescence traces for each neuron.\n",
        "          - 'bkgd_trace': Background trace.\n",
        "    hypers : Any\n",
        "        Hyperparameters related to the model (not directly used in plotting).\n",
        "    plot_bkgd : bool, optional\n",
        "        Whether to include background plots. Default is True.\n",
        "    indices : Optional[torch.Tensor], optional\n",
        "        Indices of neurons to plot; if None, all neurons are plotted.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Generates a separate plot for each neuron (and background, if requested) showing both the spatial footprint\n",
        "    and its corresponding fluorescence trace.\n",
        "    \"\"\"\n",
        "    U = params[\"footprints\"].reshape(-1, height, width)\n",
        "    u0 = params[\"bkgd_footprint\"].reshape(height, width)\n",
        "    C = params[\"traces\"]\n",
        "    c0 = params[\"bkgd_trace\"]\n",
        "    N, T = C.shape\n",
        "\n",
        "    if indices is None:\n",
        "        indices = torch.arange(N)\n",
        "\n",
        "    def _plot_factor(footprint: Float[torch.Tensor, \"height width\"],\n",
        "                     trace: Float[torch.Tensor, \"num_frames\"],\n",
        "                     title: str) -> None:\n",
        "        \"\"\"\n",
        "        Plot a neuron's footprint and its corresponding fluorescence trace.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        footprint : torch.Tensor\n",
        "            2D tensor representing the spatial footprint of a neuron,\n",
        "            with shape (height, width).\n",
        "        trace : torch.Tensor\n",
        "            1D tensor containing the fluorescence trace over time,\n",
        "            with shape (num_frames,).\n",
        "        title : str\n",
        "            Title for the subplot.\n",
        "        \"\"\"\n",
        "        fig, ax1 = plt.subplots(1, 1, figsize=(12, 6))\n",
        "        vlim = abs(footprint).max()\n",
        "        h = ax1.imshow(footprint, vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
        "        ax1.set_title(title)\n",
        "        ax1.set_axis_off()\n",
        "\n",
        "        # add a colorbar of the same height\n",
        "        divider = make_axes_locatable(ax1)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
        "        plt.colorbar(h, cax=cax)\n",
        "\n",
        "        ax2 = divider.append_axes(\"right\", size=\"150%\", pad=\"75%\")\n",
        "        ts = torch.arange(T) / FPS\n",
        "        ax2.plot(ts, trace, color=palette[0], lw=2)\n",
        "        ax2.set_xlabel(\"time (sec)\")\n",
        "        ax2.set_ylabel(\"fluorescence trace\")\n",
        "\n",
        "    if plot_bkgd:\n",
        "        _plot_factor(u0, c0, \"background\")\n",
        "\n",
        "    for k in indices:\n",
        "        _plot_factor(U[k], C[k], \"neuron {}\".format(k))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYIIcByKs1n3"
      },
      "source": [
        "### Movie of the data\n",
        "\n",
        "It takes a minute to render the animation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D2xrCAsr8fv"
      },
      "outputs": [],
      "source": [
        "# Play the motion corrected movie.\n",
        "play(data, speedup=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe29mYhmIMX_"
      },
      "source": [
        "## Part 1: Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDt8V-AMwwG-"
      },
      "source": [
        "### Problem 1a: Estimate the noise at each pixel and standardize\n",
        "\n",
        "We'll use a simple heuristic to estimate the noise. With slow calcium responses, most of the high frequency content (e.g. above 8Hz) should be noise.  Since Gaussian noise has a flat spectrum (we didn't prove this but it's a useful fact to know!), the standard deviation of the high frequency signal should tell us the noise at lower frequencies as well.\n",
        "\n",
        "In this problem, use [`butter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html) and [`sosfilt`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.sosfilt.html) to high-pass filter the data at 8Hz with a 10-th order Butterworth filter over the time axis (`axis=2`). Then compute the standard deviation for each pixel using `torch.std` and the `dim` keyword argument to get the standard deviation over time for each pixel.\n",
        "\n",
        "Finally, standardize the data by dividing each pixel by its standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCeekO586_y_"
      },
      "outputs": [],
      "source": [
        "# High-pass filter the data at 8Hz using a Butterworth filter.\n",
        "# That should filter out the calcium transients and give a\n",
        "# reasonable estimate of the noise.\n",
        "\n",
        "###\n",
        "# YOUR CODE BELOW\n",
        "sos = butter(N=..., Wn=..., btype=..., output='sos', fs=...)\n",
        "noise = sosfilt(...)\n",
        "noise = torch.tensor(noise, dtype=torch.float32) # convert to tensor\n",
        "sigmas = torch.std(...)\n",
        "###\n",
        "assert sigmas.shape == (height, width)\n",
        "\n",
        "# Plot the noise standard deviation for each pixel\n",
        "plt.imshow(sigmas, vmin=0)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Estimated noise per pixel\")\n",
        "plt.colorbar(label=\"noise std. deviation\")\n",
        "\n",
        "# Standardize the data by dividing each frame by the standard deviation\n",
        "std_data = data / sigmas[:, :, None]\n",
        "\n",
        "# Check that we got the same answer\n",
        "assert torch.isclose(sigmas.mean(), torch.tensor(23.4807), atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpldwqRXd7Ug"
      },
      "source": [
        "### Problem 1b: Find peaks in the local correlation matrix\n",
        "\n",
        "\n",
        "**Step 1**\n",
        "To find candidate neurons, look for places in the image where nearby pixels are highly correlated with one another.\n",
        "\n",
        "The correlation between pixels $(i,j)$ and $(k,\\ell)$ is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\rho_{ijk\\ell} = \\frac{1}{T} \\sum_{t=1}^T z_{ijt} z_{k\\ell t},\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z_{ijt} = \\frac{x_{ijt} - \\bar{x}_{ij}}{\\sigma_{ij}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "denotes the z-scored data (assume it is zero-padded), $x_{ijt}$ is the fluorescence at pixel $(ij)$ and frame $t$, $\\bar{x}_{ij}$ is the average fluorescence at that pixel over time, and $\\sigma_{ij}$ is the standard deviation of fluorescence in that pixel. You've already computed the noise level $\\sigma_{ij}$ for each pixel and you computed $x_{ijt} / \\sigma_{ij}$ in Problem 1a. To compute $z$, simply subtract the mean of the standardized data.\n",
        "\n",
        "Now define the local correlation at pixel $(i,j)$ to be the average correlation with its neighbors to the north, south, east, and west:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\bar{\\rho}_{ij} = \\tfrac{1}{4} \\left(\\rho_{ij,i+1,j} +  \\rho_{ij,i-1,j} + \\rho_{ij,i,j+1} + \\rho_{ij,i,j-1}\\right).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "If $(i,j)$ is a border cell, assume the correlation with out-of-bounds neighbors is zero.\n",
        "\n",
        "**Step 2**\n",
        "Use the [`gaussian_filter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter.html) function with a standard deviation `sigma=NEURON_WIDTH/4` to smooth the local correlations.\n",
        "\n",
        "**Step 3**\n",
        "Find peaks in the smoothed local correlations using [`peak_local_max`](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_peak_local_max.html),\n",
        "which we imported from the `skimage.feature` package. Set a `min_distance`\n",
        "of 2 and play with the `threshold_abs` to get 30 neurons, which we think is a reasonable estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2enApj0YILuR"
      },
      "outputs": [],
      "source": [
        "# First z-score the data\n",
        "zscored_data = std_data - std_data.mean(dim=2, keepdims=True)\n",
        "\n",
        "###\n",
        "# YOUR CODE BELOW\n",
        "\n",
        "# Compute the local correlation by summing correlations with\n",
        "# neighboring pixels\n",
        "local_correlations = torch.zeros((height, width))\n",
        "local_correlations[:, :-1] += torch.mean(...) # W\n",
        "local_correlations[:,  1:] += torch.mean(...) # E\n",
        "local_correlations[:-1, :] += torch.mean(...) # S\n",
        "local_correlations[1:,  :] += torch.mean(...) # N\n",
        "local_correlations /= 4\n",
        "\n",
        "# Smooth the local correlations with a Gaussian filter of width 1/4\n",
        "# the width of a typical neuron.\n",
        "filtered_correlations = gaussian_filter(...)\n",
        "\n",
        "# Finally, find peaks in the smoothed local correlations using\n",
        "# `peak_local_max`. Set a `min_distance` of 2 and play with the\n",
        "# `threshold_abs` to get 30 neurons, which we think is a reasonable estimate.\n",
        "peaks = peak_local_max(...)\n",
        "\n",
        "#\n",
        "###\n",
        "\n",
        "num_neurons = len(peaks)\n",
        "print(\"Found\", num_neurons, \"candidate neurons\")\n",
        "plot_problem_1d(local_correlations, filtered_correlations, peaks)\n",
        "assert num_neurons == 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSL2cR3msZW0"
      },
      "source": [
        "### Problem 1c [Short Answer]: Explain this heuristic\n",
        "\n",
        "Why are peaks in the local correlations indicative of neurons? Why did you filter the correlations? What would happen if you didn't use the Gaussian filter, or you used a Gaussian filter of a larger width?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmZruDzhICIA"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmmKumdT3puz"
      },
      "source": [
        "### Problem 1d: Initialize the footprints\n",
        "\n",
        "It's easer to initialize the footprints in 2D, even though we will eventually ravel the video frames and footprints into vectors. Initialize the 2D footprint to,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "u_{k,i,j} \\propto \\mathcal{N}\\left(\\begin{bmatrix}i \\\\ j \\end{bmatrix} \\,\\bigg|\\, \\begin{bmatrix} \\mu_{k,i} \\\\ \\mu_{k,j} \\end{bmatrix}, \\frac{w}{4} I \\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\mu_{k} \\in \\mathbb{R}^2$ is the location of the peak for neuron $k$ and $w$ is the width of a typical neuron. These are the `peaks` you computed in the previous problem.\n",
        "\n",
        "There's a simple trick to initialize the footprints: convolve a Gaussian filter with a matrix that is zeros everywhere except for a one at the location of the peak. The `gaussian_filter` function with `sigma` set to `NEURON_WIDTH/4` will do this for you.\n",
        "\n",
        "Finally, normalize the footprints so that $\\|\\mathbf{u}_k\\|=1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwZVoSQg3rtX"
      },
      "outputs": [],
      "source": [
        "footprints = torch.zeros((num_neurons, height, width))\n",
        "\n",
        "###\n",
        "# Initialize the footprints as described above\n",
        "# YOUR CODE BELOW\n",
        "\n",
        "###\n",
        "\n",
        "# Check that they're unit norm\n",
        "assert torch.allclose(torch.linalg.norm(footprints, axis=(1,2)), torch.tensor(1.0))\n",
        "\n",
        "# Plot the superimpose footprints\n",
        "plt.imshow(footprints.sum(axis=0), cmap=\"Greys_r\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"superimposed footprints\")\n",
        "_ = plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UK8Qav3MlZ"
      },
      "source": [
        "### Problem 1e: Initialize the background\n",
        "Set the spatial background factor $\\mathbf{u}_0$ equal to the **median of the standardized data** and set the temporal background factor to $\\mathbf{c}_{0} = \\mathbf{1}_T$. The median should be more robust to the large spikes than the mean is. Then normalize by dividing $\\mathbf{u}_0$ by its norm $\\|\\mathbf{u}_0\\|_2$ and multiplying $\\mathbf{c}_0$ by $\\|\\mathbf{u}_0\\|_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw9ozwR72-3b"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Initialize the background footprint as described above\n",
        "# YOUR CODE BELOW\n",
        "bkgd_footprint = ...\n",
        "bkgd_trace = ...\n",
        "\n",
        "# Rescale so that spatial background is norm 1\n",
        "...\n",
        "###\n",
        "\n",
        "# Plot the background factor\n",
        "plt.imshow(bkgd_footprint)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"background footprint $u_0$\")\n",
        "plt.colorbar()\n",
        "\n",
        "assert torch.isclose(bkgd_footprint.mean(), torch.tensor(0.0056), atol=1e-4)\n",
        "assert torch.isclose(bkgd_trace.mean(), torch.tensor(358.4527), atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX4z3cZmmUhI"
      },
      "source": [
        "### Initialize the traces\n",
        "\n",
        "We'll initialize the traces for Part 2 by computing the residual, projecting it onto each footprint in order, and updating the residual by subtracting off each neuron's contribution.\n",
        "\n",
        "If we've done a good job initializing, the traces should show clear spikes and the noise should be roughly in the range $[-3, +3]$ since the data is standardized to have standard deviation 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlkWcoBnmUKC"
      },
      "outputs": [],
      "source": [
        "# This code takes about a minute to run\n",
        "residual = std_data - torch.einsum('ij,t->ijt', bkgd_footprint, bkgd_trace)\n",
        "traces = torch.zeros((num_neurons, num_frames))\n",
        "for k in trange(num_neurons):\n",
        "    traces[k] = torch.einsum('ij,ijt->t', footprints[k], residual)\n",
        "    residual -= torch.einsum('ij,t->ijt', footprints[k], traces[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lazpULasKwrI"
      },
      "outputs": [],
      "source": [
        "# Plot trace for a single neuron\n",
        "k = 3\n",
        "plt.plot(traces[k], label=\"trace\")\n",
        "plt.hlines([-3, 3], 0, num_frames,\n",
        "        colors='r', linestyles=':', zorder=10,\n",
        "        label=\"noise level\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "plt.title(\"neuron {}\".format(k))\n",
        "\n",
        "# check that we got the same answer using the parameters from parts 1a-1e.\n",
        "assert torch.isclose(traces[3].mean(), torch.tensor(2.3482))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WOpyizUs6Ah"
      },
      "source": [
        "## Part 2: Deconvolving spikes from calcium traces\n",
        "\n",
        "In this part you'll use [CVXpy](https://www.cvxpy.org/) to deconvolve the calcium traces by solving a convex optimization problem. CVX is a \"Python-embedded modeling language for convex optimization problems,\" as the website says.  It provides an easy-to-use interface for translating convex optimization problems into code and easy access to a variety of underlying solvers. The key objects are:\n",
        "- `cvx.Variable` objects, which specify the variables you wish to optimize with respect to,\n",
        "- `cvx.Minimize` objects, which let you specify the objective you wish to minimize,\n",
        "- `cvx.Problem` objects, which combine an objective and a set of constraints.\n",
        "\n",
        "CVX also has lots of helper functions like\n",
        "- `cvx.sum_squares`, which computes the sum of squares of an array, and\n",
        "- `cx.norm`, which computes norms of the specified order.\n",
        "\n",
        "The following example is modified from the CVXpy homepage, linked above. It solves a least-squares problem with box constraints and compares the constrained and unconstrained solutions.\n",
        "\n",
        "_Note: CVXpy is typically used with NumPy arrays, but it can operate on PyTorch tensors too. We'll just have to remember to convert the results back into tensors in subsequent steps._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtp80R3qKdzz"
      },
      "outputs": [],
      "source": [
        "# A simple CVX example...\n",
        "\n",
        "# Problem data.\n",
        "torch.manual_seed(1)\n",
        "A = dist.Normal(0.0, 1.0).sample((30, 20))\n",
        "b = dist.Normal(0.0, 1.0).sample((30,))\n",
        "\n",
        "# Construct the problem.\n",
        "x = cvx.Variable(20)\n",
        "objective = cvx.Minimize(cvx.sum_squares(A @ x - b))\n",
        "constraints = [0 <= x, x <= 1]\n",
        "prob = cvx.Problem(objective, constraints)\n",
        "\n",
        "# The optimal objective value is returned by `prob.solve()`.\n",
        "# The optimal value for x is stored in `x.value`.\n",
        "result = prob.solve(verbose=False)\n",
        "\n",
        "# Plot the constrained optimum vs the unconstrained.\n",
        "plt.fill_between([0, 19], 0, 1, color='k', alpha=0.1, hatch='x',\n",
        "                 label=\"constraint set\")\n",
        "plt.plot(x.value, '-o', label=\"$0 \\leq x \\leq 1$\")\n",
        "plt.plot(torch.linalg.lstsq(A, b, rcond=None)[0], '-', marker='.',\n",
        "         label=\"unconstrained\")\n",
        "plt.xlim(0, 19)\n",
        "plt.ylim(-1, 1.0)\n",
        "plt.xlabel(\"$n$\")\n",
        "plt.ylabel(\"$x_n^\\star$\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T43ScVIr1N_j"
      },
      "source": [
        "### Problem 2a: Solve the convex optimization problem in dual form with CVX\n",
        "\n",
        "In this part of the lab you'll use CVXpy to maximize the log joint probability in its **dual form:**\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\hat{\\mathbf{c}}_k, \\hat{b}_k = \\text{arg min}_{\\mathbf{c}_k, b_k} \\; \\|\\mathbf{G} \\mathbf{c}_k\\|_1\n",
        "    \\quad \\text{subject to } \\quad\n",
        "    \\|\\boldsymbol{\\mu}_k - \\mathbf{c}_k - b_k\\|_2^2 &\\leq \\theta^2, \\; \\mathbf{G} \\mathbf{c}_k \\geq 0,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{\\mu}_k = \\mathbf{u}_k^\\top \\mathbf{R} \\in \\mathbb{R}^T$ is the target for neuron $k$ and\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\mathbf{G} &=\n",
        "    \\begin{bmatrix}\n",
        "    1             &               &        &        \\\\\n",
        "    -e^{-1/\\tau} & 1             &        &        \\\\\n",
        "    0             & -e^{-1/\\tau} & 1      &        \\\\\n",
        "                  & 0             & \\ddots & \\ddots \\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "is the first order difference matrix. The spike amplitudes (i.e. jumps in the fluorescence) are given by $\\mathbf{a}_k = \\mathbf{G} \\mathbf{c}_k$, so you can think about the optimization problem as minimizing the $L^1$ norm of the jumps subject to a non-negativity constraint and an upper bound on the $L^2$ norm of the difference between the target $\\boldsymbol{\\mu}_k$ and the trace $\\mathbf{c}_k$.\n",
        "\n",
        "**Note** that this is a slight modification of the problem presented in class:\n",
        "1. Here we've added a bias term $b_k$, which will be helpful in cases where the target has a nonzero baseline.  Accounting for this possibility will lead to more robust estimates of the calcium traces.\n",
        "2. In class we presented the constraint $\\|\\boldsymbol{\\mu}_k - \\mathbf{c}_k -b\\|_2 \\leq \\theta$. CVXpy does a much better job at solving these \"second order cone programs,\" so in practice that's what you should do! For this problem, however, you'll square both sides, as written in the objective above. Squaring doesn't change the constraint set, but it will make it easier to compare to the \"primal\" form you'll solve in Problem 2d and 2e.\n",
        "\n",
        "We argued that a reasonable guess for the norm threshold is $\\theta = (1+\\epsilon) \\sigma \\sqrt{T}$.  For large $T$ and good estimates of the target, we should be able to set $\\epsilon$ pretty small.  Here, we'll use a fairly liberal upper bound since we're working with a short dataset and a poor initial guess.\n",
        "\n",
        "One of the great things about CVXpy is that it **works with SciPy's sparse matrices.** For example, you can use `scipy.sparse.diags` to construct the $\\mathbf{G}$ matrix. Under the hood, the solver will leverage the sparsity to run in linear time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itd6oizT04im"
      },
      "outputs": [],
      "source": [
        "def deconvolve(\n",
        "    trace: Union[np.ndarray, Float[torch.Tensor, \"T\"]],\n",
        "    noise_std: float = 1.0,\n",
        "    epsilon: float = 1.0,\n",
        "    tau: float = GCAMP_TIME_CONST_SEC * FPS,\n",
        "    full_output: bool = False,\n",
        "    verbose: bool = False) -> Union[torch.Tensor, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Deconvolve a noisy calcium trace (aka \"target\") by solving a convex optimization problem.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trace : np.ndarray [should work with torch.Tensor too]\n",
        "        A T numpy array containing the noisy trace.\n",
        "    noise_std : float\n",
        "        Scalar noise standard deviation ($\\sigma$). Default is 1.0.\n",
        "    epsilon : float\n",
        "        Extra slack for the norm constraint (typically > 0 and certainly > -1).\n",
        "         Default is 1.0.\n",
        "    tau : float\n",
        "        The time constant of the calcium indicator decay.\n",
        "        Default is GCAMP_TIME_CONST_SEC * FPS.\n",
        "    full_output : bool\n",
        "        If True, returns a dictionary with the deconvolved trace and additional information;\n",
        "        otherwise returns only the deconvolved trace. Default is False.\n",
        "    verbose : bool\n",
        "        Flag to pass to the CVX solver to print more information.\n",
        "        Default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Union[torch.Tensor, Dict[str, Any]]\n",
        "        Either a torch.Tensor representing the deconvolved trace, or a dictionary containing:\n",
        "          - \"trace\": the deconvolved trace,\n",
        "          - \"baseline\": the optimized baseline,\n",
        "          - \"result\": the solver result,\n",
        "          - \"amplitudes\": the convolution G @ trace,\n",
        "          - \"lagrange_multiplier\": the dual value for the norm constraint.\n",
        "    \"\"\"\n",
        "    assert trace.ndim == 1\n",
        "    T = len(trace)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    # Initialize the variable we're optimizing over\n",
        "    c = cvx.Variable(...)\n",
        "    b = cvx.Variable(...)\n",
        "\n",
        "    # Create the sparse matrix G with 1 on the diagonal and\n",
        "    # -e^{-1/\\tau} on the first lower diagonal\n",
        "    G = ...\n",
        "\n",
        "    # set the threshold to (1+\\epsilon) \\sigma \\sqrt{T}\n",
        "    theta = ...\n",
        "\n",
        "    # Define the objective function\n",
        "    objective = cvx.Minimize(...)\n",
        "\n",
        "    # Set the constraints.\n",
        "    # PUT THE NORM CONSTRAINT FIRST, THEN THE NON-NEGATIVITY CONSTRAINT!\n",
        "    constraints = [..., ...]\n",
        "\n",
        "    # Construct the problem\n",
        "    prob = cvx.Problem(..., ...)\n",
        "    ###\n",
        "\n",
        "    # Solve the optimization problem.\n",
        "    try:\n",
        "        # First try the default solver then revert to SCS if it fails.\n",
        "        result = prob.solve(verbose=verbose)\n",
        "    except Exception as e:\n",
        "        print(\"Default solver failed with exception:\")\n",
        "        print(e)\n",
        "        print(\"Trying 'solver=SCS' instead.\")\n",
        "        # if this still fails we give up!\n",
        "        result = prob.solve(verbose=verbose, solver=\"SCS\")\n",
        "\n",
        "    # Make sure the result is finite (i.e. it found a feasible solution)\n",
        "    if torch.isinf(torch.tensor(result)):\n",
        "        raise Exception(\"solver failed to find a feasible solution!\")\n",
        "\n",
        "    all_results = dict(\n",
        "        trace=c.value,\n",
        "        baseline=b.value,\n",
        "        result=result,\n",
        "        amplitudes=G @ c.value,\n",
        "        lagrange_multiplier=constraints[0].dual_value[0]\n",
        "    )\n",
        "    assert torch.numel(torch.tensor(constraints[0].dual_value)) == 1, \\\n",
        "        \"Make sure your first constraint is on the norm of the residual.\"\n",
        "\n",
        "    return all_results if full_output else c.value\n",
        "\n",
        "# Solve the deconvolution problem for one neuron\n",
        "k = 3              # this neuron has particularly high SNR\n",
        "noise_std = 1.0     # \\sigma is 1 since we standardized the data\n",
        "epsilon = 1.0       # start with a generous tolerance of 2 \\sigma \\sqrt{T}\n",
        "dual_results = deconvolve(traces[k],\n",
        "                          noise_std=noise_std,\n",
        "                          epsilon=epsilon,\n",
        "                          full_output=True,\n",
        "                          verbose=True)\n",
        "\n",
        "# Plot\n",
        "plt.plot(traces[k], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"],\n",
        "         color=palette[0], lw=2, label=\"deconvolved\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "_ = plt.title(\"neuron {}\".format(k))\n",
        "\n",
        "# Check your answer\n",
        "assert torch.isclose(torch.tensor(dual_results[\"result\"], dtype=torch.float32),\n",
        "                     torch.tensor(563.4), 1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwRBOndN1H9R"
      },
      "source": [
        "### Plot solutions for a range of $\\epsilon$ (and hence of $\\theta$)\n",
        "\n",
        "\n",
        "Compute and plot the solutions (in separate figures) for a range of $\\epsilon$ values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQDls6w15Nu"
      },
      "outputs": [],
      "source": [
        "epsilons = [0, 0.25, 0.5, 0.75, 1, 2]\n",
        "for epsilon in epsilons:\n",
        "    print(\"solving with epsilon = \", epsilon)\n",
        "\n",
        "    # deconvolve with this epsilon\n",
        "    dual_results = deconvolve(traces[k],\n",
        "                              noise_std=noise_std,\n",
        "                              epsilon=epsilon,\n",
        "                              full_output=True,\n",
        "                              verbose=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure()\n",
        "    plt.plot(traces[k], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "    plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"],\n",
        "            color=palette[0], lw=2, label=\"\".format(epsilon))\n",
        "    plt.legend(loc=\"upper left\", fontsize=10)\n",
        "    plt.xlim(0, num_frames)\n",
        "    plt.xlabel(\"time (frames)\")\n",
        "    plt.ylabel(\"fluorescence\")\n",
        "    _ = plt.title(\"neuron {} $\\epsilon$ = {:.2f}\".format(k, epsilon))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67HdszCk3j8A"
      },
      "source": [
        "### Problem 2b [Short Answer]: Explain these results\n",
        "\n",
        "How does the solution change as you increase $\\epsilon$ and thereby increase $\\theta$? Why?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leNpf4GwYr6Q"
      },
      "source": [
        "\n",
        "_Your answer here_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht1ZnUYjTxNO"
      },
      "source": [
        "### Problem 2c [Math]: Relate the dual form to the primal\n",
        "\n",
        "Replacing the upper bound on the squared norm in Problem 2a with its Lagrangian, we obtain the following \"primal\" form of the problem:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\hat{\\mathbf{c}}_k, \\hat{b}_k = \\text{arg min}_{\\mathbf{c}_k, b_k} \\; \\eta (\\|\\boldsymbol{\\mu}_k - \\mathbf{c}_k - b_k\\|_2^2 - \\theta^2) + \\|\\mathbf{G} \\mathbf{c}_k\\|_1\n",
        "    \\quad \\text{subject to } \\quad  \\mathbf{G} \\mathbf{c}_k \\geq 0,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the Lagrange multiplier.\n",
        "\n",
        "**Show** that this is equivalent to maximizing the log joint (with a baseline $b_k$)\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\mathbf{c}}_k, \\hat{b}_k = \\text{arg max}_{\\mathbf{c}_k, b_k} \\mathcal{L}(\\mathbf{c}_k, b_k) &= -\\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_k - \\mathbf{c}_k - b_k\\|_2^2 - \\lambda_k\\|\\mathbf{G} \\mathbf{c}_k\\|_1 \\quad \\text{subject to } \\quad  \\mathbf{G} \\mathbf{c}_k \\geq 0\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "by **solving for the value of $\\lambda_k$** (in terms of $\\eta$ and $\\sigma$) that makes these problems equivalent.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5BzMcy5Yr6Q"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7WCkiEyk8Yw"
      },
      "source": [
        "### Problem 2d: Solve the problem in primal form with $\\lambda_k$ set to match the dual\n",
        "\n",
        "Solve the primal problem with CVX using the amplitude rate hyperparameter $\\lambda_k$ that you solved for in Problem 2d and the optimal Lagrange multiplier $\\eta$ output in Problem 2a. In code,\n",
        "\n",
        "```\n",
        "dual_results[\"lagrange_multiplier\"]   # this is \\eta\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YPMnNTxThwi"
      },
      "outputs": [],
      "source": [
        "def deconvolve_primal(\n",
        "    trace: Union[np.ndarray, Float[torch.Tensor, \"T\"]],\n",
        "    amplitude_rate: float,\n",
        "    noise_std: float = 1.0,\n",
        "    tau: float = GCAMP_TIME_CONST_SEC * FPS,\n",
        "    verbose: bool = True,\n",
        "    full_output: bool = False) -> Union[torch.Tensor, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Deconvolve a noisy calcium trace (aka \"target\") by solving\n",
        "     a convex optimization problem in the primal form.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trace : np.ndarray [should work with torch.Tensor too]\n",
        "        A T numpy array containing the noisy trace.\n",
        "    amplitude_rate : float\n",
        "        Non-negative rate (inverse scale) parameter $\\lambda$.\n",
        "    noise_std : float, optional\n",
        "        Scalar noise standard deviation ($\\sigma$). Default is 1.0.\n",
        "    tau : float, optional\n",
        "        The time constant of the calcium indicator decay.\n",
        "         Default is GCAMP_TIME_CONST_SEC * FPS.\n",
        "    verbose : bool, optional\n",
        "        Flag to pass to the CVX solver to print additional information.\n",
        "        Default is True.\n",
        "    full_output : bool, optional\n",
        "        If True, returns a dictionary with the deconvolved trace and additional information; otherwise, returns only the deconvolved trace. Default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Union[torch.Tensor, Dict[str, Any]]\n",
        "        Either a torch.Tensor representing the deconvolved trace, or a dictionary containing:\n",
        "            - \"trace\": the deconvolved trace,\n",
        "            - \"baseline\": the optimized baseline,\n",
        "            - \"result\": the solver result,\n",
        "            - \"amplitudes\": the computed spike amplitudes (G @ c.value).\n",
        "    \"\"\"\n",
        "    assert trace.ndim == 1\n",
        "    T = len(trace)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    # Initialize the variable we're optimizing over\n",
        "    c = cvx.Variable(...)\n",
        "    b = cvx.Variable(...)\n",
        "\n",
        "    # Create the sparse matrix G with 1 on the diagonal and\n",
        "    # -e^{-1/\\tau} on the first lower diagonal\n",
        "    G = ...\n",
        "\n",
        "    # Define the objective function\n",
        "    objective = cvx.Minimize(...)\n",
        "    constraints = [...]\n",
        "    prob = cvx.Problem(...)\n",
        "    ###\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    result = prob.solve(verbose=verbose)\n",
        "    if torch.isinf(torch.tensor(result)):\n",
        "        raise Exception(\"solver failed to find a feasible solution!\")\n",
        "\n",
        "    all_results = dict(\n",
        "        trace=c.value,\n",
        "        baseline=b.value,\n",
        "        result=result,\n",
        "        amplitudes=G @ c.value\n",
        "    )\n",
        "    return all_results if full_output else c.value\n",
        "\n",
        "\n",
        "# Solve the deconvolution problem in the dual form\n",
        "k = 3               # this neuron has particularly high SNR\n",
        "noise_std = 1.0     # \\sigma is 1 since we standardized the data\n",
        "epsilon = 1.0       # start with a generous tolerance \\epsilon = 1\n",
        "dual_results = deconvolve(traces[k],\n",
        "                          noise_std=noise_std,\n",
        "                          epsilon=epsilon,\n",
        "                          full_output=True,\n",
        "                          verbose=True)\n",
        "\n",
        "###\n",
        "# Convert the optimal Lagrange multiplier returned in Problem 2a\n",
        "# to a hyperparameter $\\lambda_n$ that sets the rate (inverse scale)\n",
        "# of the exponential prior on spike amplitudes. The multiplier `eta` is in\n",
        "# `dual_results['lagrange_multiplier']` and \\sigma is set by `noise_std`.\n",
        "#\n",
        "# YOUR CODE BELOW\n",
        "amplitude_rate = ...\n",
        "###\n",
        "\n",
        "\n",
        "# Solve the problem in primal form\n",
        "primal_results = deconvolve_primal(traces[k],\n",
        "                                   amplitude_rate=amplitude_rate,\n",
        "                                   verbose=True,\n",
        "                                   full_output=True)\n",
        "\n",
        "# Plot raw, primal, and dual optimal trace for neuron n\n",
        "plt.plot(traces[k], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"],\n",
        "         color=palette[0], ls='-', lw=2, label=\"dual\")\n",
        "plt.plot(primal_results[\"trace\"] + primal_results[\"baseline\"],\n",
        "         color=palette[1], ls='-', lw=1, label=\"primal\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "plt.title(\"neuron {}\".format(k))\n",
        "\n",
        "# Make sure the traces are the same!\n",
        "primal_diff = abs(dual_results[\"trace\"] - primal_results[\"trace\"]).max()\n",
        "print(\"primal and dual solutions match to absolute value: {:.4f}\".format(primal_diff))\n",
        "assert torch.allclose(torch.tensor(dual_results[\"trace\"]),\n",
        "                      torch.tensor(primal_results[\"trace\"]),\n",
        "                      atol=1e-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdfARactBOPx"
      },
      "source": [
        "### Compute all deconvolved traces and plot them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZjkStbq1CJk"
      },
      "outputs": [],
      "source": [
        "# Deconvolve each trace and concatenate the results\n",
        "deconvolved_traces = torch.zeros_like(traces)\n",
        "amplitudes = torch.zeros_like(traces)\n",
        "for neuron in trange(num_neurons):\n",
        "    all_results = deconvolve(traces[neuron], epsilon=0.9, full_output=True)\n",
        "    deconvolved_traces[neuron] = torch.tensor(all_results[\"trace\"],\n",
        "                                              dtype=torch.float32)\n",
        "    amplitudes[neuron] = torch.tensor(all_results[\"amplitudes\"],\n",
        "                                      dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqOsxWYlSVuc"
      },
      "outputs": [],
      "source": [
        "plot_problem_2(traces, deconvolved_traces, amplitudes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5amaIbDtyp"
      },
      "source": [
        "## Part 3: Demix and deconvolve the calcium imaging video\n",
        "\n",
        "In this part you'll write the updates for MAP estimation in the constrained non-negative matrix factorization model.\n",
        "\n",
        "As in the notes and slides, we will operate on the **flattened** data and residuals by reshaping the frames into 1d vectors.\n",
        "\n",
        "**Note** that unlike CNMF (Pnevmatikakis et al, 2016), we're not going to constrain the footprints to be non-negative. Instead, we'll just assume they are normalized, since that's a bit easier to and it makes a clearer connection to the spike sorting algorithms from the previous lab. It would be a simple extension to enforce non-negativity, and the course notes describe how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWHVDWvYsnt"
      },
      "source": [
        "### Flatten the pixel dimensions and package the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh1ClO8B8EeH"
      },
      "outputs": [],
      "source": [
        "flat_data = std_data.reshape(-1, num_frames)\n",
        "flat_footprints = footprints.reshape(num_neurons, -1)\n",
        "flat_bkgd_footprint = bkgd_footprint.reshape(-1)\n",
        "\n",
        "# Package the paramters into a dictionary\n",
        "params = dict(\n",
        "    traces=torch.zeros((num_neurons, num_frames)),  # C\n",
        "    bkgd_trace=bkgd_trace,                          # c_0\n",
        "    footprints=flat_footprints,                     # U\n",
        "    bkgd_footprint=flat_bkgd_footprint              # u_0\n",
        ")\n",
        "\n",
        "# Move the data and params to the GPU\n",
        "flat_data = flat_data.to(device)\n",
        "for key in params.keys():\n",
        "    params[key] = params[key].to(device)\n",
        "\n",
        "# The hyperparameters specify the number of neurons,\n",
        "# the noise standard deviation ($\\sigma = 1$ since we standardized the data),\n",
        "# the prior variance of the background trace (something really large),\n",
        "# and the tolerance for our norm constrain ($\\epsilon$).\n",
        "hypers = dict(\n",
        "    num_neurons=num_neurons,\n",
        "    noise_std=1.0,\n",
        "    bkgd_trace_var=1e6,\n",
        "    epsilon=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILoHcgb7yRTr"
      },
      "source": [
        "### Problem 3a: Write a function to compute the log likelihood given the residual\n",
        "\n",
        "The log likelihood is\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log p(\\mathbf{X} \\mid \\mathbf{U}, \\mathbf{C}) &=\n",
        "\\sum_{n=1}^N \\sum_{t=1}^T \\log \\mathcal{N}\\left(x_{n,t} \\,\\bigg|\\, \\sum_{k=0}^K u_{k,n} c_{k,t}, \\sigma^2 \\right) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Write a function to compute the log likelihood given the precomputed residual $\\mathbf{R} = \\mathbf{X} - \\mathbf{U} \\mathbf{C}^\\top $.\n",
        "\n",
        "_Hint: Use `dist.Normal`'s `log_prob` function._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SafHuShnofsh"
      },
      "outputs": [],
      "source": [
        "def log_likelihood_residual(\n",
        "    residual: Float[torch.Tensor, \"N T\"],\n",
        "    hypers: Dict[str, Any]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Evaluate the log joint probability of the data given the precomputed residual.\n",
        "\n",
        "    The residual represents the difference between the observed data and the contributions\n",
        "    of the neurons and background, i.e. Y - U^T C - u_0 c_0^T.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residual : torch.Tensor\n",
        "        A tensor of shape (N, T) containing the residual noise after subtracting the neuron\n",
        "        and background contributions.\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        A scalar tensor containing the normalized log likelihood (i.e. divided by the number\n",
        "        of residual elements).\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE HERE\n",
        "    lp = ...\n",
        "    ###\n",
        "    return lp / residual.numel()\n",
        "\n",
        "# check it on the flat data (as if C and c_0 were zero)\n",
        "assert torch.isclose(\n",
        "    log_likelihood_residual(flat_data, hypers),\n",
        "    torch.tensor(-4.6855), atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpZC0-0zW1B"
      },
      "source": [
        "### Problem 3b: Optimize a trace\n",
        "\n",
        "Optimize a single neuron's trace using the `deconvolve` function you wrote in Problem 2a. The target is $\\boldsymbol{\\mu}_k = \\mathbf{u}_k^\\top \\mathbf{R}_k$ where $R_k$ is the residual for this neuron. The residual is given as input to this function.\n",
        "\n",
        "**Note:** In your final version, make sure you have `verbose=False` so that the final code doesn't print a bunch of unnecessary stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa3C6cfh_Ukj"
      },
      "outputs": [],
      "source": [
        "def _update_trace(\n",
        "    neuron: int,\n",
        "    residual: Float[torch.Tensor, \"N T\"],\n",
        "    params: Dict[str, torch.Tensor],\n",
        "    hypers: Dict[str, Any]) -> Float[torch.Tensor, \"T\"]:\n",
        "    \"\"\"\n",
        "    Update a single neuron's trace by deconvolving the corresponding residual.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron : int\n",
        "        The index of the neuron to update.\n",
        "    residual : torch.Tensor\n",
        "        A tensor of shape (N, T) containing the residual for this neuron.\n",
        "    params : dict\n",
        "        A dictionary of parameters\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The updated trace for the neuron as a tensor of shape (T,).\n",
        "    \"\"\"\n",
        "    footprint = params[\"footprints\"][neuron]\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    target = ...\n",
        "    target = target.to(\"cpu\")    # Move to CPU so CVXPy can use it\n",
        "    trace = deconvolve(...)\n",
        "    ###\n",
        "\n",
        "    # Move trace back to device before returning\n",
        "    trace = torch.tensor(trace, device=device, dtype=torch.float32)\n",
        "    assert torch.all(torch.isfinite(trace))\n",
        "    return trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4HW8kqD1s3v"
      },
      "source": [
        "### Problem 3c: Optimize a footprint\n",
        "\n",
        "Optimize a single neuron's footprint by setting it to $\\mathbf{u}_k = \\frac{\\mathbf{R} \\mathbf{c}_k}{\\|\\mathbf{R} \\mathbf{c}_k\\|}$  where $\\mathbf{R}$ is the given residual and $\\mathbf{c}_k$ is the neuron's trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ_xw9YY1sHW"
      },
      "outputs": [],
      "source": [
        "def _update_footprint(\n",
        "    neuron: int,\n",
        "    residual: Float[torch.Tensor, \"N T\"],\n",
        "    params: Dict[str, torch.Tensor],\n",
        "    hypers: Dict[str, Any]) -> Float[torch.Tensor, \"F\"]:\n",
        "    \"\"\"\n",
        "    Update a single neuron's footprint.\n",
        "\n",
        "    Based on the current neuron trace (from params[\"traces\"]) and the residual,\n",
        "    compute a new, normalized spatial footprint for the neuron.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron : int\n",
        "        The index of the neuron to update.\n",
        "    residual : torch.Tensor\n",
        "        A tensor of shape (N, T) representing the residual for this neuron.\n",
        "    params : dict\n",
        "        A dictionary of parameters containing \"traces\" and \"footprints\".\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The updated, normalized footprint as a tensor of shape (F,).\n",
        "        Its norm should be 1.\n",
        "    \"\"\"\n",
        "    trace = params[\"traces\"][neuron]\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    footprint = ...\n",
        "    ###\n",
        "\n",
        "    assert torch.all(torch.isfinite(footprint))\n",
        "    assert torch.linalg.norm(footprint).isclose(torch.tensor(1.0))\n",
        "    return footprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYtR8VGAsg3V"
      },
      "source": [
        "### Problem 3d: Optimize the background\n",
        "\n",
        "Optimize the background trace by projecting the residual onto the background footprint and shrinking the result slightly,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{c}_0 = \\left(\\frac{\\varsigma_0^2}{\\sigma^2 + \\varsigma_0^2}\\right) \\mathbf{u}_0^\\top \\mathbf{R}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{R} = \\mathbf{X} - \\sum_{k=1}^K \\mathbf{u}_k\\mathbf{c}_k^\\top $ is the background residual and $\\varsigma_0^2$ is the prior variance on the background trace. (See the course notes for a derivation.)\n",
        "\n",
        "Update the background footprint by setting it to,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{u}_0 = \\frac{\\mathbf{R} \\mathbf{c}_0}{\\|\\mathbf{R} \\mathbf{c}_0\\|}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3lvVemWsWuO"
      },
      "outputs": [],
      "source": [
        "def _update_bkgd_trace(\n",
        "    residual: Float[torch.Tensor, \"N T\"],\n",
        "    params: Dict[str, torch.Tensor],\n",
        "    hypers: Dict[str, Any]) -> Float[torch.Tensor, \"T\"]:\n",
        "    \"\"\"\n",
        "    Update the background trace $c_0$\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residual : torch.Tensor\n",
        "        A tensor of shape (N, T) containing the residual for the background.\n",
        "    params : dict\n",
        "        A dictionary of parameters including \"bkgd_footprint\" and \"bkgd_trace\".\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters with keys including \"noise_std\" and \"bkgd_trace_var\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The updated background trace as a tensor of shape (T,).\n",
        "    \"\"\"\n",
        "    sigmasq = hypers[\"noise_std\"]**2\n",
        "    sigmasq_prior = hypers[\"bkgd_trace_var\"]\n",
        "    footprint = params[\"bkgd_footprint\"]\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    shrink_factor = ...\n",
        "    target = ...\n",
        "    ###\n",
        "\n",
        "    # update the latent variables in place\n",
        "    return shrink_factor * target\n",
        "\n",
        "def _update_bkgd_footprint(\n",
        "    residual: Float[torch.Tensor, \"N T\"],\n",
        "    params: Dict[str, torch.Tensor],\n",
        "    hypers: Dict[str, Any]) -> Float[torch.Tensor, \"F\"]:\n",
        "    \"\"\"\n",
        "    Update the background footprint $u_0$.\n",
        "\n",
        "    Using the current background trace from params and the residual,\n",
        "    compute a new background footprint and normalize it to unit norm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residual : torch.Tensor\n",
        "        A tensor of shape (N, T) containing the residual for the background.\n",
        "    params : dict\n",
        "        A dictionary of parameters including \"bkgd_trace\" and \"bkgd_footprint\".\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The updated, normalized background footprint as a tensor of shape (F,).\n",
        "    \"\"\"\n",
        "    bkgd_trace = params[\"bkgd_trace\"]\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    bkgd_footprint = ...\n",
        "    ###\n",
        "\n",
        "    assert torch.all(torch.isfinite(bkgd_footprint))\n",
        "    assert torch.linalg.norm(bkgd_footprint).isclose(torch.tensor(1.0))\n",
        "    return bkgd_footprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9FL_P01plBx"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Now we'll put these steps together into the MAP estimation algorithm. It's very similar to what you implemented in Lab 2. It amounts to:\n",
        "- Initialize the residual $\\mathbf{R} = \\mathbf{X} - \\mathbf{U} \\mathbf{C}^\\top$\n",
        "- Repeat until convergence:\n",
        "    - For each neuron $k=1,\\ldots,K$:\n",
        "        - Update the residual to $\\mathbf{R} = \\mathbf{R} + \\mathbf{u}_k \\mathbf{c}_k^\\top$\n",
        "        - Update the trace $\\mathbf{c}_k$ by applying your `deconvolve` function from Part 2a to the target $\\boldsymbol{\\mu}_k = \\mathbf{u}_k^\\top \\mathbf{R}$\n",
        "        - Update the footprint to $\\mathbf{u}_k = \\frac{\\mathbf{R} \\mathbf{c}_k}{\\|\\mathbf{R} \\mathbf{c}_k\\|}$\n",
        "        - Downdate the residual to $\\mathbf{R} = \\mathbf{R} - \\mathbf{u}_k \\mathbf{c}_k^\\top$ using the new footprint and trace\n",
        "    - Update the background:\n",
        "        - Update the residual to $\\mathbf{R} = \\mathbf{R} + \\mathbf{u}_0 \\mathbf{c}_0^\\top$\n",
        "        - Set the background trace to $\\mathbf{c}_0 = \\frac{\\varsigma_0^2}{\\sigma^2 + \\varsigma_0^2} \\mathbf{u}_0^\\top \\mathbf{R}$ where $\\varsigma_0^2$ is the prior variance of the background trace. (We will set it to be very large so that we barely shrink the background trace.)\n",
        "        - Set the background footprint to $\\mathbf{u}_0 = \\frac{\\mathbf{R} \\mathbf{c}_0}{\\|\\mathbf{R} \\mathbf{c}_0\\|}$\n",
        "        - Downdate the residual to $\\mathbf{R} = \\mathbf{R} - \\mathbf{u}_0 \\mathbf{c}_0^\\top$ using the new background footprint and trace.\n",
        "    - Compute the log likelihood using the residual $\\mathbf{R}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oIpGwwk2dKJ"
      },
      "outputs": [],
      "source": [
        "def map_estimate(\n",
        "    flat_data: Float[torch.Tensor, \"F T\"],\n",
        "    params: Dict[str, torch.Tensor],\n",
        "    hypers: Dict[str, Any],\n",
        "    num_iters: int = 10,\n",
        "    tol: float = 2e-4) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Fit the CNMF model using coordinate ascent to obtain the maximum a posteriori estimate.\n",
        "\n",
        "    The algorithm alternates between updating neuron traces, neuron footprints, and background\n",
        "    components until convergence based on the log likelihood of the residual.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    flat_data : torch.Tensor\n",
        "        A tensor with shape (F, T) representing the flattened imaging data.\n",
        "    params : dict\n",
        "        A dictionary of parameters including:\n",
        "            - \"traces\": Tensor of shape (num_neurons, T)\n",
        "            - \"bkgd_trace\": Tensor of shape (T,)\n",
        "            - \"footprints\": Tensor of shape (num_neurons, F)\n",
        "            - \"bkgd_footprint\": Tensor of shape (F,)\n",
        "    hypers : dict\n",
        "        A dictionary of hyperparameters containing:\n",
        "            - \"num_neurons\": int, the number of neurons.\n",
        "            - \"noise_std\": float, the noise standard deviation.\n",
        "            - \"bkgd_trace_var\": float, the prior variance for the background trace.\n",
        "            - \"epsilon\": float, the tolerance for the norm constraint.\n",
        "    num_iters : int, optional\n",
        "        The maximum number of iterations for coordinate ascent.\n",
        "        Default is 10.\n",
        "    tol : float, optional\n",
        "        The convergence tolerance on the change in log likelihood.\n",
        "        Default is 2e-4.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[torch.Tensor, dict]\n",
        "        A tuple containing:\n",
        "            - A tensor of log likelihood values over iterations.\n",
        "            - The updated parameter dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    # make a fancy reusable progress bar for the inner loops over neurons.\n",
        "    outer_pbar = trange(num_iters)\n",
        "    inner_pbar = trange(hypers[\"num_neurons\"])\n",
        "    inner_pbar.set_description(\"updating neurons\")\n",
        "\n",
        "    # initialize the residual\n",
        "    residual = torch.clone(flat_data)\n",
        "    residual -= params[\"footprints\"].T @ params[\"traces\"]\n",
        "    residual -= torch.outer(params[\"bkgd_footprint\"], params[\"bkgd_trace\"])\n",
        "\n",
        "    # track log likelihoods over iterations\n",
        "    lls = [log_likelihood_residual(residual, hypers)]\n",
        "    outer_pbar.set_description(\"LL: {:.4f}\".format(lls[-1]))\n",
        "\n",
        "    # coordinate ascent\n",
        "    for itr in outer_pbar:\n",
        "\n",
        "        # update neurons one at a time\n",
        "        inner_pbar.reset()\n",
        "        for k in range(hypers[\"num_neurons\"]):\n",
        "            # update the residual (add $u_k c_k^\\top$)\n",
        "            residual += torch.outer(params[\"footprints\"][k], params[\"traces\"][k])\n",
        "\n",
        "            # update the trace and footprint with the residual\n",
        "            params[\"traces\"][k] = _update_trace(k, residual, params, hypers)\n",
        "            params[\"footprints\"][k] = _update_footprint(k, residual, params, hypers)\n",
        "\n",
        "            # downdate the residual (subtract $u_k c_k^\\top$)\n",
        "            residual -= torch.outer(params[\"footprints\"][k], params[\"traces\"][k])\n",
        "\n",
        "            # step the progress bar\n",
        "            inner_pbar.update()\n",
        "\n",
        "        # update the background\n",
        "        residual += torch.outer(params[\"bkgd_footprint\"], params[\"bkgd_trace\"])\n",
        "        params[\"bkgd_trace\"] = _update_bkgd_trace(residual, params, hypers)\n",
        "        params[\"bkgd_footprint\"] = _update_bkgd_footprint(residual, params, hypers)\n",
        "        residual -= torch.outer(params[\"bkgd_footprint\"], params[\"bkgd_trace\"])\n",
        "\n",
        "        # compute the log likelihood\n",
        "        lls.append(log_likelihood_residual(residual, hypers))\n",
        "        outer_pbar.set_description(\"LL: {:.4f}\".format(lls[-1]))\n",
        "\n",
        "        # check for convergence\n",
        "        if abs(lls[-1] - lls[-2]) < tol:\n",
        "            print(\"Convergence detected!\")\n",
        "            break\n",
        "\n",
        "    return torch.stack(lls), params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh1v8NhFFNGI"
      },
      "source": [
        "### Fit it!\n",
        "\n",
        "This should take about 2 minutes with a GPU backend.\n",
        "\n",
        "**Note**: With the default setting of $\\epsilon$, you will likely see the following warning:\n",
        "\n",
        "```\n",
        "Default solver failed with exception:\n",
        "Solver 'ECOS' failed. Try another solver, or solve with verbose=True for more information.\n",
        "Trying 'solver=SCS' instead.\n",
        "/usr/local/lib/python3.8/dist-packages/cvxpy/problems/problem.py:1337: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
        "```\n",
        "\n",
        "When this happens, the SCS solver can take around 20 seconds to complete. For me, it only happens when updating neuron 28 in the first epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDmCcwkh39ZL"
      },
      "outputs": [],
      "source": [
        "# Fit it!\n",
        "lls, params = map_estimate(flat_data, params, hypers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVgPqRMtE7FB"
      },
      "outputs": [],
      "source": [
        "# Plot the log likelihoods\n",
        "plt.plot(lls.to(\"cpu\"), '-o',)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.xlim(-.1, len(lls) - .9)\n",
        "plt.ylabel(\"Log Likelihood\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szs4eIDYOxXn"
      },
      "source": [
        "### Plot the inferred footprints and traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brOpyqsT1yq-"
      },
      "outputs": [],
      "source": [
        "# Yes, we know we're creating a lot of figures...\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "for key in params.keys():\n",
        "    params[key] = params[key].to(\"cpu\")\n",
        "\n",
        "plot_problem_3(flat_data, params, hypers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnIbegX2O4-j"
      },
      "source": [
        "### Make a movie of the data, reconstruction, and residual\n",
        "\n",
        "Show a movie with the data, reconstruction, and residual side by side. If all goes well, the data should show a nice, clean movie of spiking neurons and the residual should mostly look like white noise. In practice, you'll probably still see some evidence of neurons in the residual, suggesting that the model still isn't perfect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IHou0U_HGg0q"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Reconstruct the data and compute the residual.\n",
        "# Then make a movie of the data, reconstruction, and residual\n",
        "# side-by-side.\n",
        "#\n",
        "flat_recon = params[\"footprints\"].T @ params[\"traces\"]\n",
        "flat_recon += torch.outer(params[\"bkgd_footprint\"], params[\"bkgd_trace\"])\n",
        "flat_residual = flat_data.to(\"cpu\") - flat_recon\n",
        "\n",
        "# Reshape into image stacks and concatenate along axis=1.\n",
        "movie = torch.cat([\n",
        "    flat_data.reshape(height, width, -1).to(\"cpu\"),\n",
        "    flat_recon.reshape(height, width, -1),\n",
        "    flat_residual.reshape(height, width, -1),\n",
        "    ], dim=1)\n",
        "\n",
        "# Play the movie\n",
        "play(movie, speedup=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWM2YDdr8uDU"
      },
      "source": [
        "## Part 4: Discussion\n",
        "\n",
        "Hopefully you were successful in separating the neurons from the background and noise! Let's take a minute to reflect on the model and results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vWLCLr4Yr6V"
      },
      "source": [
        "### Problem 4a\n",
        " We mentioned a few times that actual CNMF implementations also constrain the footprints to be non-negative. Without this constraint, you probably found in the plots above (before Problem 3e) that some of these footprints contain negative values. Why is this unrealistic and what are the consequences of omitting this constraint?\n",
        "\n",
        " ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POulG5iSYr6V"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxinOlsAYr6V"
      },
      "source": [
        "### Problem 4b\n",
        "\n",
        "You probably noticed that the background has lots of rings in it, like little Cheerios. What could cause that effect?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN5CQQX-Yr6V"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8fkZRViYr6V"
      },
      "source": [
        "### Problem 4c\n",
        "We assumed that all neurons share the same time constant $\\tau$. Is that reasonable? Without doing any math, describe how you would try to learn per-neuron time constants.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe_4RP9VYr6V"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDPnFrGYr6V"
      },
      "source": [
        "### Problem 4d\n",
        "Do you think we can infer the number of underlying action potentials from the amplitude of the jumps in the calcium traces?  Why or why not?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb4tdmViYr6V"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn_g-oTAOxHd"
      },
      "source": [
        "## Author contributions\n",
        "\n",
        "Please write a short paragraph describing each authors contributions. Also please include any tools used or other people consulted.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs6R4-3YO24F"
      },
      "source": [
        "_Your response here_"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "67HdszCk3j8A",
        "Ht1ZnUYjTxNO"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b28c5bd4ee93d765ebe901023d5522822fb8ad083dac3187c5545022f913719"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}